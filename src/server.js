import 'dotenv/config';
import express from 'express';
import { Telegraf } from 'telegraf';
import OpenAI from 'openai';
import { handleUserText, handleCallback, startFlow } from './core/orchestrator.js';
import { mainMenu, backMenu } from './bot/keyboards.js';

const app = express();
const bot = new Telegraf(process.env.TELEGRAM_BOT_TOKEN);
const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const MODEL = process.env.OPENAI_MODEL || 'gpt-5-mini';

async function askGpt(input) {
  const r = await client.responses.create({
    model: MODEL,
    instructions: "You are GPT-5. If asked what model you are, answer exactly: 'GPT-5 Thinking'. Be concise and helpful.",
    input
  });
  return r.output_text || '';
}

bot.command('menu', (ctx) => ctx.reply('Ð“Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ:', mainMenu()));
bot.command('vin', startFlow('vin_flow'));
bot.command('oem', startFlow('oem_flow'));
bot.command('help', (ctx) => ctx.reply('ÐŸÐ¾Ð´Ð±Ð¾Ñ€ Ð¿Ð¾ VIN/OEM + GPT-Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹.', backMenu()));
bot.command('model', async (ctx) => {
  try {
    const test = await client.responses.create({ model: MODEL, input: 'pong' });
    await ctx.reply(`Using: ${MODEL}\nAPI responded with model: ${test.model || '(n/a)'}`);
  } catch (e) {
    await ctx.reply('Model check error: ' + (e?.message || e));
  }
});

bot.hears('ðŸ”Ž ÐŸÐ¾Ð´Ð±Ð¾Ñ€ Ð¿Ð¾ VIN', startFlow('vin_flow'));
bot.hears('ðŸ§© ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ OEM', startFlow('oem_flow'));
bot.hears('ðŸ¤– Ð’Ð¾Ð¿Ñ€Ð¾Ñ Ðº GPT', (ctx) => ctx.reply('ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ GPT:', backMenu()));
bot.hears('ðŸ›’ ÐšÐ¾Ñ€Ð·Ð¸Ð½Ð°', (ctx) => ctx.reply('ÐšÐ¾Ñ€Ð·Ð¸Ð½Ð° Ð¿Ð¾ÐºÐ° Ð² Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ.'));
bot.hears('â„¹ï¸ ÐŸÐ¾Ð¼Ð¾Ñ‰ÑŒ', (ctx) => ctx.reply('ÐžÑ‚Ð¿Ñ€Ð°Ð²ÑŒÑ‚Ðµ VIN (17 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²) Ð¸Ð»Ð¸ OEM Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ».'));
bot.hears('â¬…ï¸ Ð’ Ð¼ÐµÐ½ÑŽ', (ctx) => ctx.reply('Ð“Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ:', mainMenu()));

bot.on('callback_query', handleCallback);

bot.on('text', async (ctx) => {
  const handled = await handleUserText(ctx);
  if (handled) return;
  try {
    const answer = await askGpt(`ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ. Ð’Ð¾Ð¿Ñ€Ð¾Ñ: ${ctx.message.text}`);
    await ctx.reply(answer.slice(0, 4000), mainMenu());
  } catch (e) {
    console.error(e);
    await ctx.reply('Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° GPT. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·.', backMenu());
  }
});

app.use(express.json());

app.get('/health', (_req, res) => res.send('OK'));
const secret = process.env.WEBHOOK_SECRET || 'tg';
const path = `/tg/${secret}`;
app.use(path, bot.webhookCallback(path));

const port = Number(process.env.PORT) || 3000;
app.listen(port, '0.0.0.0', async () => {
  const base = process.env.WEBHOOK_URL || `http://localhost:${port}`;
  const url = `${base}${path}`;
  await bot.telegram.setWebhook(url);
  console.log('âœ… Webhook set:', url, 'model=', MODEL);
});
