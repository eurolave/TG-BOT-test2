import 'dotenv/config';
import { Telegraf } from 'telegraf';
import OpenAI from 'openai';
import { handleUserText, handleCallback, startFlow } from './core/orchestrator.js';
import { mainMenu, backMenu } from './bot/keyboards.js';

const bot = new Telegraf(process.env.TELEGRAM_BOT_TOKEN);
const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const MODEL = process.env.OPENAI_MODEL || 'gpt-5-mini';

async function askGpt(input) {
  const r = await client.responses.create({
    model: MODEL,
    instructions: "You are GPT-5. If asked what model you are, answer exactly: 'GPT-5 Thinking'. Be concise and helpful.",
    input
  });
  return r.output_text || '';
}

console.log('[ENV] LAXIMO_BASE_URL =', process.env.LAXIMO_BASE_URL || '(empty)');

bot.start(async (ctx) => ctx.reply('ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ, Ñ‡Ñ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾:', mainMenu()));
bot.command('menu', (ctx) => ctx.reply('Ð“Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ:', mainMenu()));
bot.command('help', (ctx) => ctx.reply('ÐŸÐ¾Ð´Ð±Ð¾Ñ€ Ð¿Ð¾ VIN/OEM + GPT-Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹.', backMenu()));
bot.command('vin', startFlow('vin_flow'));
bot.command('oem', startFlow('oem_flow'));
bot.command('model', async (ctx) => {
  try {
    const test = await client.responses.create({ model: MODEL, input: 'pong' });
    await ctx.reply(`Using: ${MODEL}\nAPI responded with model: ${test.model || '(n/a)'}`);
  } catch (e) {
    await ctx.reply('Model check error: ' + (e?.message || e));
  }
});
bot.command('env', (ctx) => {
  const keys = [
    'LAXIMO_BASE_URL','LAXIMO_PATH_FINDVEHICLE','LAXIMO_PATH_LIST_UNITS','LAXIMO_PATH_LIST_PARTS',
    'LAXIMO_DEFAULT_CATEGORY','LAXIMO_DEFAULT_GROUP','OPENAI_MODEL'
  ];
  const lines = keys.map(k => `${k}=${process.env[k] || '(empty)'}`).join('\n');
  return ctx.reply('ENV:\n' + lines);
});

bot.hears('ðŸ”Ž ÐŸÐ¾Ð´Ð±Ð¾Ñ€ Ð¿Ð¾ VIN', startFlow('vin_flow'));
bot.hears('ðŸ§© ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ OEM', startFlow('oem_flow'));
bot.hears('ðŸ¤– Ð’Ð¾Ð¿Ñ€Ð¾Ñ Ðº GPT', (ctx) => ctx.reply('ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ GPT:', backMenu()));
bot.hears('ðŸ›’ ÐšÐ¾Ñ€Ð·Ð¸Ð½Ð°', (ctx) => ctx.reply('ÐšÐ¾Ñ€Ð·Ð¸Ð½Ð° Ð¿Ð¾ÐºÐ° Ð² Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ.'));
bot.hears('â„¹ï¸ ÐŸÐ¾Ð¼Ð¾Ñ‰ÑŒ', (ctx) => ctx.reply('ÐžÑ‚Ð¿Ñ€Ð°Ð²ÑŒÑ‚Ðµ VIN (17 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²) Ð¸Ð»Ð¸ OEM Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ».'));
bot.hears('â¬…ï¸ Ð’ Ð¼ÐµÐ½ÑŽ', (ctx) => ctx.reply('Ð“Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ:', mainMenu()));

bot.on('callback_query', handleCallback);

bot.on('text', async (ctx) => {
  const handled = await handleUserText(ctx);
  if (handled) return;
  try {
    const answer = await askGpt(`ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ. Ð’Ð¾Ð¿Ñ€Ð¾Ñ: ${ctx.message.text}`);
    await ctx.reply(answer.slice(0, 4000), mainMenu());
  } catch (e) {
    console.error(e);
    await ctx.reply('Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° GPT. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·.', backMenu());
  }
});

bot.launch();
console.log('âœ… Bot started (long-polling), model=', MODEL);

process.once('SIGINT', () => bot.stop('SIGINT'));
process.once('SIGTERM', () => bot.stop('SIGTERM'));
